# Initial Survey on Reasoning and Efficiency: Recent Advances in Deployable Language Models

This repository curates and critically reviews recent work on efficient and robust inference for reasoning-capable language models, with a focus on quantization, test-time adaptation, and scaling strategies.

## Summary Table

| Paper | Focus | Key Contribution | Limitations / Open Questions |
|------|-------|------------------|------------------------------|
| Deng et al. (2508.02180) | Quantized TTA | ZOA: zeroth-order adaptation for quantized models | Assumes domain shift detection; limited architecture scope |
| ... | ... | ... | ... |

## Paper Reviews

### 1. Test-Time Model Adaptation for Quantized Neural Networks (arXiv:2508.02180)
- **Problem**: ...
- **Approach**: ...
- **Insight**: ...
- **Critique**: ...
- **Link**: [arXiv](
https://doi.org/10.48550/arXiv.2508.02180)

...

## Emerging Themes & Research Directions
- Theme 1: Quantization disproportionately harms reasoning (vs. standard tasks)
- Theme 2: Test-time diversity (via temperature or traces) unlocks latent capability
- Theme 3: Efficiency â‰  just speed; must preserve reasoning fidelity
