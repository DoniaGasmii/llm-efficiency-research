# Initial Survey on Reasoning and Efficiency: Recent Advances in Deployable Language Models

This repository curates and critically reviews recent work on efficient and robust inference for reasoning-capable language models, with a focus on quantization, test-time adaptation, and scaling strategies.

## Summary Table

| Paper | Focus | Key Contribution | Limitations / Open Questions |
|------|-------|------------------|------------------------------|
| Deng et al. (2508.02180) | Quantized TTA | ZOA: zeroth-order adaptation for quantized models | Assumes domain shift detection; limited architecture scope |
| ... | ... | ... | ... |

## Paper Reviews

### 1. Test-Time Model Adaptation for Quantized Neural Networks 
- **Problem**: ...
- **Approach**: ...
- **Insight**: ...
- **Critique**: ...
- **Link**: [arXiv](https://doi.org/10.48550/arXiv.2508.02180)

### 2. Efficient Reasoning Models: A Survey 
- **Problem**: ...
- **Approach**: ...
- **Insight**: ...
- **Critique**: ...
- **Link**: [arXiv](https://arxiv.org/abs/2504.10903)

### 3. On the Role of Temperature Sampling in Test-Time Scaling 
- **Problem**: ...
- **Approach**: ...
- **Insight**: ...
- **Critique**: ...
- **Link**: [arXiv](https://arxiv.org/abs/2510.02611)

### 4. Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models 
- **Problem**: ...
- **Approach**: ...
- **Insight**: ...
- **Critique**: ...
- **Link**: [arXiv](https://arxiv.org/abs/2504.04823)

### 5. Mixed-Precision Quantization for Language Models: Techniques and Prospects
- **Problem**: ...
- **Approach**: ...
- **Insight**: ...
- **Critique**: ...
- **Link**: [arXiv](https://arxiv.org/abs/2510.16805)


...

## Emerging Themes & Research Directions
- Theme 1: Quantization disproportionately harms reasoning (vs. standard tasks)
- Theme 2: Test-time diversity (via temperature or traces) unlocks latent capability
- Theme 3: Efficiency â‰  just speed; must preserve reasoning fidelity
